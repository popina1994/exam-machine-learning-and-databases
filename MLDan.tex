\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{makeidx}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{bm}
\usepackage[numbers]{natbib}
\usepackage{tikz}
\usepackage{amsthm}
\usetikzlibrary{calc}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{thmtools, thm-restate}
\usepackage{xcolor}

\author{Djordje Zivanovic}
\title{\textbf{Warsaw PhD Open Course: From Joins to Aggregates and
Optimization Problems - Exam solutions}}
\begin{document}
\maketitle
\bigskip
%Chapter 1 in Ockendon \& Tayler: Applied PDE's  
%{\it Maths Institute Lecture Notes}. 	
%{\bf Exercises 1, 2, 4, 5}.
\newcommand{\transpose}[1]{#1 ^ \text{T}}
\newcommand{\mat}[1]{\textbf{\text{{#1}}}}
\newcommand{\vect}[1]{\mat{#1}}
\newcommand{\view}[3]{\delta R_*(a', b') \cdot V_{S_{#1}T_{#2}W_{#3}}(b', a')}
\newcommand{\gradient}[1]{\nabla_{\vect{#1}}}
\newcommand{\der}[1]{\frac{\partial}{\partial #1}}
\newcommand{\numberthis}{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\logfun}{\sigma(\transpose{\vect{x}}_i \vect{w})}
\newcommand{\softmaxi}{\frac{e^{z_i}}{\sum_{l=1}^Ce^{z_l}}}
\newcommand{\layersep}{2.5cm}
\newcommand{\layersepH}{6cm}
\newcommand{\layersepP}{7cm}
\newcommand{\layersepB}{9cm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\newtheorem{conjecture}[theorem]{Conjecture}
\begin{enumerate}
\item[1]
Let $Q(\mathbf{A_1} \cup \mathbf{A_2} \cup \cdots \cup \mathbf{A_n})$ denote a join query on relations $R_1(\mathbf{A_1})=R_1(A_1, A_2), \cdots R_n(\mathbf{A_n})=R_n(A_n, A_1)$.

Let $\mathcal{H}=(\mathcal{V}, \mathcal{E})$ denote a corresponding hypegraph to the join query, then $\mathcal{V} = \{A_1, .., A_n\}$ and $\mathcal{E} =\{\{A_1, A_2\}, ..., \{A_n, A_1\}\}$.
\begin{enumerate}
\item[1.1]
Let $x_{R_i}$ denote the weight of an hyperedge (relation) $R_i$ in the join query $Q$. In addition let $cover_i$ denote the set of relations corresponding to edges that are part of the cover for the variable $i$. The fractional edge cover number $\rho^{*}(Q)$ is the cost of an optimal solution to the linear program:
\begin{eqnarray}
\text{minimize} \sum_{i\in [n]} x_{R_i}  \label{eq:minimize} \\
\text{subject to} \sum_{rel_j \in cover_i} x_{rel_j} \geq 1, i \in [n]  \label{eq:inequalities}\\
x_{R_i} \geq 0, i \in [n] \nonumber
\end{eqnarray}

By a definition of a cover, only edges incident  to variable are part of its cover. Thus, in our case $cover_i =\{ R_{i-1}, R_i \}$, where $R_0$ is $R_n$ in the case of $A_1$. If we rewrite the inequalities \ref{eq:inequalities} we will get:
\begin{eqnarray}
x_{R_1} +  x_{R_n} \geq 1 \nonumber \\
x_{R_1} +  x_{R_2} \geq 1\nonumber \\
\vdots \nonumber \\
x_{R_{n-1}} +  x_{R_n} \geq 1 \nonumber
\end{eqnarray}
By summing inequalities we get:
$$2\left(\sum_{i\in [n]} x_{R_i}\right) \geq n$$
\end{enumerate}
This means the minimized function $\rho^*(Q)$ cannot  be less than $\frac{n}{2}$. This is achievable if we set $x_{R_i}=\frac{1}{2}, i \in [n]$. We can see that this solution satisfies all constraints in the linear program. Finally, $\rho^*(Q)=\frac{n}{2}$. 
\\
A width of a specific hypertree decomposition is defined as a maximum among all edge covers for each of the nodes of the hypertree. A hypertree width (htw) for the specific query Q is defined as a minimum width among all possible widths of hypertree decompositions of a query Q. 
\\
A fractional width of a specific hypertree decomposition is defined as a maximum among all fractional edge covers for each of the nodes of the hypertree. A fractional hypertree width (fhtw) for the specific query Q is defined as a minimum width among all possible widths of hypertree decompositions of a query Q.
The only difference is that in the case of fractional hypertree width we use fractional edge covers instead of edge covers. \\
In the case of $n=2$ $fhtw(Q) = htw(Q) = 1$. For $n >= 3$ the hypertree decomposition on the figure \ref{fig:fthw} is the optimal one. From picture we can figure out that $htw(Q)=2$ in all cases, while $fhtw(Q)=2$ for $n>3$, but for $n=3$ it is $\frac{3}{2}$. 
\\
The decomposition on the figure \ref{fig:fthw} is optimal because we have to "propagate" one variable through nodes of the hypertree in order to fulfill the condition that all hyperedges of hypergraph are contained in the nodes of hypertree. Adding more than three variables inside a node of hypertree would increase (fractional) width of a hypertree decomoposition because we would need to cover at least two variables that are not in the same relation.  (Fractional) edge cover number for each of nodes of hypegraph inside nodes of hyperetree is $2$ because $A_1 A_iA_{i-1}$ can be covered with $x_{R_i}=1, x_{R_1}=1$. The only exception to the previous case is $n=3$ where fractional edge cover of $\frac{3}{2}$ is possible. 
\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{scope}[every node/.style={circle,thick,draw}]
    \node (C) at (2.5,2.5) {$A_1 A_2$};
    \node (D) at (2.5,0) {$A_1 A_2 A_3$};
    \node (E) at (2.5,-2.5) {$A_1 A_{n-1}A_{n}$};
\end{scope}
\begin{scope}
    \path  (C) edge node {$\vdots$    \, \,} (D);
    \path[]  (D) edge node {$\vdots$    \, \,} (E);
\end{scope}
\end{tikzpicture} 
\end{center}
\caption{Hypertree decomposition that corresponds to fhtw}
\label{fig:fthw}
\end{figure}

\item[1.2]
A size of listing representation is $\mathcal{O}(N^{\rho^*(Q)})=\mathcal{O}(N^{\frac{n}{2}})$ and can be computed in time $\mathcal{O}(N^{\rho^*(Q)})=\mathcal{O}(N^{\frac{n}{2}})$. Let $|D|$ represent a size the of training data set which is the number of tuples in the join of all relations $R_1, R_2, ..., R_n$. Let $\mat{X}$ represent training data set consisting of tuples of the join with an additional intercept column with values only 1. Let $\vect{y}$ represent labels of training data corresponding to tuples of join and let $\pmb{\theta}$ be a column vector of training parameters for all features and a bias. In addition, let $\lambda$ represent hyperparemeter of $\ell_2$ regularizer.
A loss function with added $\ell_2$ regularizer is :
\begin{eqnarray}
2|D|\mathcal{L}(\mat{X},\vect{y}, \mathbf{\theta})  &=& (\vect{y} - \mat{X}\mathbf{\theta})^2 \nonumber + \lambda |D| ||\mathbf{\theta}||_2^2\\
			&=& \transpose{(\vect{y} - \mat{X}\mathbf{\theta})}(\vect{y} - \mat{X}\mathbf{\theta}) + \lambda |D| ||\mathbf{\theta}||_2^2 \nonumber \\
&=&\transpose{\vect{y}}\vect{y} - \transpose{\vect{y}}\mat{X}\mathbf{\theta}
- \transpose{\mathbf{\theta}}\transpose{\mat{X}} \vect{y}
+ \transpose{\mathbf{\theta}}\transpose{\mat{X}}\mat{X}\mathbf{\theta} + \lambda |D| |\mathbf{\theta}||_2^2 \nonumber \\
&=&
\transpose{\vect{y}}\vect{y} - 2 \transpose{\vect{y}}\mat{X}\mathbf{\theta}
+ \transpose{\mathbf{\theta}}\transpose{\mat{X}}\mat{X}\mathbf{\theta} + \lambda |D| \transpose{\mathbf{\theta}}\mathbf{\theta}
\label{eq:loss}
\end{eqnarray}
In the equation \ref{eq:loss} we used the property that transpose of a scalar is the same scalar. All equations before that are simple vector multiplications and additions. 
Finally: 
$$\mathcal{L}(\mat{X},\vect{y}, \mathbf{\theta}) = 
\frac{1}{2|D|} \transpose{\vect{y}}\vect{y} - \frac{1}{|D|} \transpose{\vect{y}}\mat{X}\mathbf{\theta}
+ \frac{1}{2|D|}\transpose{\mathbf{\theta}}\transpose{\mat{X}}\mat{X}\mathbf{\theta} + 
\frac{\lambda}{2} \transpose{\mathbf{\theta}}\mathbf{\theta}
$$
If we derive the equation \ref{eq:loss} we get the gradient of the loss function for one point:
\begin{eqnarray}
\nabla_{\mathbf{\theta}} \mathcal{L}(\mat{X},\vect{y}, \mathbf{\theta}) &=& 0  - \frac{1}{|D|}\transpose{\mat{X}}\vect{y} + \frac{1}{|D|}\transpose{\mat{X}}\mat{X}\mathbf{\theta} + \lambda \mathbf{\theta} \nonumber \\
&=& \frac{1}{|D|}\transpose{\mat{X}}\mat{X}\mathbf{\theta} - \frac{1}{|D|}\transpose{\mat{X}}\vect{y} + \lambda \mathbf{\theta}\nonumber
\end{eqnarray}
Here we used properties $\gradient{\vect{x}}(\transpose{\vect{x}}\mat{A}\vect{x})=\mat{A}\vect{x} + \transpose{\mat{A}}\vect{x}$, $\gradient{\vect{x}}(\transpose{\vect{x}}\vect{x})=2\vect{x}$, $\gradient{\vect{x}}(\transpose{\vect{c}} \vect{x}) = \vect{c}$. In addition we used $\transpose{\left(\vect{u} \vect{v}\right)} = \transpose{\vect{v}}\transpose{\vect{u}}$. \\
A gradient descent formula is:
$$\mathbf{\theta}_{t+1}=\mathbf{\theta}_t - \eta\nabla_{\mathbf{\theta}} \mathcal{L}(\mat{X},\vect{y}, \mathbf{\theta})$$
where $\theta_t$ represents parameters in the $t$-th iteration (we start with $\theta_0$ ) and $\eta$ represents hyperparameter of gradient learning.  
\\
After joining relations we need in each step to calculate gradient. 
We can notice that $\transpose{\mat{X}}\vect{y}$ can be cached and the complexity to do so is $\mathcal{O}(n \cdot N^\frac{n}{2})$. Similarly with $\transpose{\mat{X}}\mat{X}$, whose calculation takes $\mathcal{O}(n^2 \cdot N^\frac{n}{2})$. In each iteration we need to multiply $\transpose{\mat{X}}\mat{X}$ with $\mathbf{\theta}$ which takes $\mathcal{O}(n^2)$ time. We will ignore additions of vectors because they are one $O(n)$.   
When we add  all time complexities we get:
$$\mathcal{O}(N^\frac{n}{2} + n^2 \cdot N^\frac{n}{2} + n \cdot  N^\frac{n}{2} + n^2 \cdot t) $$
If we refined the complexity and included the fact that categorical variables behave as the vectors whose length is the same as a length of the particular domain, the complexity would become:
$$\mathcal{O}(N^\frac{n}{2} + (n_d)^2 \cdot N^\frac{n}{2} + n_d \cdot  N^\frac{n}{2} + n_d^2 \cdot t) $$
where $n_d= n - 3 +  |Dom_{A_1}|+|Dom_{A_2}| + |Dom_{A_3}|$.
\\Memory complexity after the join is
$\mathcal{O}(N^\frac{n}{2})$. $\transpose{\mat{X}}\mat{X}$ cached takes $\mathcal{O}(n^2)$ and $\transpose{\mat{X}}\vect{y}$ takes $\mathcal{O}(n)$.
When we add these complexities we get memory complexity at each iteration is:
$\mathcal{O}(n^2)$. In addition, if we include in the result the fact that we have categorical values, memory complexity is $\mathcal{O}(n_d^2)$. 

\item[1.3]
If we denote $\mathbf{\Sigma} = \frac{1}{|D|}\transpose{\mat{X}} \mat{X}, \vect{c} = \frac{1}{|D|}\transpose{\mat{X}}\vect{y}, $
and elements of these tensors, respectively, $\mathbf{\sigma}_{ij}, \vect{c}_{i}$ then FAQs for $\Sigma$ are: 
\begin{itemize}
\item When i-th feature and j-th feature are continuous values. 
$$|D| \cdot \mathbf{\sigma}_{ij}= \sum_{a_1 \in Dom(A_1)} \sum_{a_2 \in Dom(A_2)} \cdots \sum_{a_n \in Dom(A_n)} a_i \cdot a_j 
\prod_{k\in[n]} \vect{1}_{R_k(a_k, a_{k+1})}$$
\item When i-th feature is a categorical value and j-th feature is a continuous value. 
$$|D| \cdot \mathbf{\sigma}_{ij}[a_i]= \sum_{a_1 \in Dom(A_1)} \sum_{a_2 \in Dom(A_2)} \cdots 
\sum_{a_{i-1} \in Dom(A_{i-1})}
\sum_{a_{i+1} \in Dom(A_{1+1})}
\cdots
\sum_{a_n \in Dom(A_n)}  a_j 
\prod_{k\in[n]} \vect{1}_{R_k(a_k, a_{k+1})}$$
\item When i-th feature and j-th feature are categorical values. 
$$|D| \cdot \mathbf{\sigma}_{ij}[a_i, a_j]= 
\sum_{\substack{a_{1} \in \\Dom(A_{1})}}
\sum_{\substack{a_{2} \in \\Dom(A_{2})}}
\cdots 
\sum_{\substack{a_{i-1} \in \\Dom(A_{i-1})}}
\sum_{\substack{a_{i+1} \in \\Dom(A_{i+1})}}
\cdots
\sum_{\substack{a_{j-1} \in \\Dom(A_{j-1})}}
\sum_{\substack{a_{j+1} \in \\Dom(A_{j+1})}}
\cdots
\sum_{a_n \in Dom(A_n)}  
\prod_{k\in[n]} \vect{1}_{R_k(a_k, a_{k+1})}$$

\end{itemize}
If we calculate $\sigma_{ij}$ we don't need to calculate $\sigma_{ji}$ because $\Sigma$ matrix is symmetric, therefore $\sigma_{ij} =\transpose{\sigma_{ij}}$ \\
For $\vect{c}$ FAQs:
\begin{itemize}
\item When i-th feature and label are  continuous values: 
$$|D| \cdot \mathbf{c}_{i}= \sum_{a_1 \in Dom(A_1)} \sum_{a_2 \in Dom(A_2)} \cdots \sum_{a_n \in Dom(A_n)} a_i \cdot a_n 
\prod_{k\in[n]} \vect{1}_{R_k(a_k, a_{k+1})}$$
\item When i-th feature is a categorical value and label is a continuous value: 
$$|D| \cdot \mathbf{c}_{i}[a_i]= \sum_{a_1 \in Dom(A_1)} \sum_{a_2 \in Dom(A_2)} \cdots 
\sum_{a_{i-1} \in Dom(A_{i-1})}
\sum_{a_{i+1} \in Dom(A_{1+1})}
\cdots
\sum_{a_n \in Dom(A_n)}  a_n 
\prod_{k\in[n]} \vect{1}_{R_k(a_k, a_{k+1})}$$
\item When i-th feature and label are categorical values: 
$$|D| \cdot \mathbf{c}_{i}[a_i, a_n]= 
\sum_{\substack{a_{1} \in \\Dom(A_{1})}}
\sum_{\substack{a_{2} \in \\Dom(A_{2})}}
\cdots 
\sum_{\substack{a_{i-1} \in \\Dom(A_{i-1})}}
\sum_{\substack{a_{i+1} \in \\Dom(A_{i+1})}}
\cdots
\sum_{a_{n-1} \in Dom(A_{n-1})}  
\prod_{k\in[n]} \vect{1}_{R_k(a_k, a_{k+1})}$$
\end{itemize}
Just let us notice that $a_{n+1}=a_1$ for notational purposes, $i \in [n-1] \cup \{0\} $, where $0$ represents intercept column feature. In cases where one of the features is intercept feature, there is no "skip" in sums over the intercept feature.
Here we used $\mathbf{1}_E$ which is the Kronecker delta that evaulates to 1(0) whenever the event $E$ (not) holds. $R_k(a_k, a_{k+1})$ evaluates to 1(0) whenever tuple $(a_k, a_{k+1})$ is a part of relation $R_k$. 
If we wanted FAQs per definition from the slides we would:
\begin{itemize}
\item set $\psi_k=\vect{1}_{R_k(a_k, a_{k+1})}$, except in the case of $\psi_{i}$ and $\psi_j$ where the expression above is multiplied by $a_i$, $a_j$.
\item set $\oplus = \sum$
\item set $\otimes = \prod$
\item set free variables to corresponding categorical variables if there are any. For instance, in the case when there is one categorical variable a free variable would be that variable. 
\end{itemize}
According to the proposition 3.4 from \cite[page 7]{abo2018database}  complexity is:
$$
\mathcal{O}\left(|\mathcal{V}|^{2} \cdot|\mathcal{E}| \cdot \sum_{i, j \in[n-1]\cup\{0\}}\left(N^{\mathrm{faqw}(i, j)}+\left|\sigma_{i j}\right|\right) \cdot \log N\right)
 $$
 where $ \mathrm{faqw}(i, j) \leq \mathrm{fhtw}(i, j) + c - 1$ according to proposition A.9  from \cite{abo2018database}, $c$ is a maximum of categorical variables for any $\sigma_{ij}$, because they are free variables. In the case when we have only continuous variables $\mathrm{faqw} =\mathrm{fhtw}$ the previous complexity is then 
 $$
\mathcal{O}\left(|\mathcal{V}|^{2} \cdot|\mathcal{E}| \cdot \sum_{i, j \in \{4, 5, ..., n-1\}\cup\{0\}}N^{\mathrm{2}}\cdot \log N\right) = \mathcal{O}\left(n^5 N^{\mathrm{2}}\cdot \log N\right)
 $$
 This complexity can be get by using InsideOut algorithm with no free variables:
\begin{align*}
D| \cdot \mathbf{\sigma}_{ij}&=& 
\sum_{a_1 \in Dom(A_1)} \sum_{a_2 \in Dom(A_2)} \cdots \sum_{a_n \in Dom(A_n)} 
\prod_{k\in[n]} \psi_k(a_k, a_{k+1}) \\
&=& 
 \sum_{a_2 \in Dom(A_2)} \cdots \sum_{a_n \in Dom(A_n)} 
\prod_{k\in \{ 2, .., n\}} \psi_k(a_k, a_{k+1})  \cdot 
\underbrace{\left( \sum_{a_1 \in Dom(A_1)} \psi_1(a_1, a_2) \psi_n(a_1, a_n)\right)}_{\psi_{22}(a_2, a_n)} \\
&=&  \sum_{a_2 \in Dom(A_2)} \cdots \sum_{a_n \in Dom(A_n)} 
\psi_{22}(a_2, a_n) \prod_{k\in\{2,...,n-1\} } \psi_k(a_k, a_{k+1}) &&\mathcal{O}(N^2) \\
&=&  \sum_{a_3 \in Dom(A_3)} \cdots \sum_{a_n \in Dom(A_n)} 
\psi_{32}(a_3, a_n) \prod_{k\in\{3,...,n-1\} } \psi_k(a_k, a_{k+1}) &&\mathcal{O}(N^2) \\
\vdots \\
&=&  \sum_{a_{n-1} \in Dom(A_{n-1})} \sum_{a_n \in Dom(A_n)} 
\psi_{n-1 2}(a_{n-1}, a_n) \psi_{n-1}(a_n, a_{n-1})&&\mathcal{O}(N^2)\\
&=& \sum_{a_n \in Dom(A_n)} 
\psi_{n 2} &&\mathcal{O}(N)
\end{align*}
where $$\psi_{k2} = \sum_{a_{k-1} \in Dom(A_{k-1})} \psi_{k-1\, 2} (a_{k-1}, a_{n}) \psi_{k-1}(a_{k-1}, a_k), k \in \{ 3, 4 .., n \}$$
In each step where we calculated FAQ, we see that the calculation takes ${O}(N^2)$ because of the joining on a variable in each step and calculating sum, thus complexity is as it is stated in the proposition 3.4.  We could have used indicator projection in each step to remove unnecessary domains from $\psi_{k2}, k \in \{ 2, 3, ..., n\}$. \\
When we have categorical and continuous case the solution is similar, just in our calculations we would not sum over the free variable, moreover it will be on the top of the variable order. Complexity in this case is:
\begin{align*}
\mathcal{O}\left(|\mathcal{V}|^{2} \cdot|\mathcal{E}| \cdot \sum_{i\in\{1, 2, 3\}j \in \{ 4, 5 , n-1\} \cup\{0\}}\left(N^{\mathrm{faqw}(i, j)}+\left|\sigma_{i j}\right|\right) \cdot \log N\right)
&=&
\mathcal{O}\left( n^2 \cdot n \cdot 3n \left(N^2 + \left|\sigma_{i j}\right|\right) \log N \right) \\
&=&  
\mathcal{O}\left( n^4 \left(N^2 + N) \right) \log N \right)\\
&=&  
\mathcal{O}\left( n^4 N^2  \log N \right)
\end{align*}
 
When we have two categorical variables we would join relations one by one leaving categorical variables on the top of variable order. At the end join process would be the same except in the case when $\sigma_{13}(a_1, a_3)\sigma_{31}(a_3, a_1)$, where we need to calculate sum over $a_2$ as the last.   
Complexity in this case is:
\begin{align*}
\mathcal{O}\left(|\mathcal{V}|^{2} \cdot|\mathcal{E}| \cdot \sum_{i\in\{1, 2, 3\}j \in \{ 1, 2, 3\} }\left(N^{\mathrm{faqw}(i, j)}+\left|\sigma_{i j}\right|\right) \cdot \log N\right)
&=&
\mathcal{O}\left( n^2 \cdot n \cdot 9 \left(N^2 + \left|\sigma_{i j}\right|\right) \log N \right) \\
&=&  
\mathcal{O}\left( n^3 \left(N^2 + N^2) \right) \log N \right)\\
&=&  
\mathcal{O}\left( n^3 N^2  \log N \right)
\end{align*}
Finally time complexity is:
\begin{align*}
\mathcal{O}\left(|\mathcal{V}|^{2} \cdot|\mathcal{E}| \cdot \sum_{i, j \in[n-1]\cup\{0\}}\left(N^{\mathrm{faqw}(i, j)}+\left|\sigma_{i j}\right|\right) \cdot \log N\right)
&=&\mathcal{O}\left(n^5 N^{\mathrm{2}}\cdot \log N\ +  n^4 N^{\mathrm{2}}\cdot \log N +  n^3 N^2  \log N \right) \\
&=& \mathcal{O}\left(n^5 N^{\mathrm{2}}\cdot \log N\right) 
\end{align*} 
$|\sigma_{ij}|$ is limited by $|N|$ or $|N|^2$ (depending on a number of free variables), but it can be as low as $1$. 
The same is with the memory complexity where it can go up to $\mathcal{O}(n_d^2)$  because we need to list $\sigma_{ij}$. Although, it can be even $\mathcal{O}(n^2)$, depending on database. 

%  	
\item [1.4]
By properties of functional dependency $A_2 \rightarrow A_1A_3 \Leftrightarrow A_2 \rightarrow A_1, A_2\rightarrow A_3$. This means if we one-hot encode each of categories from variables $A_1, A_2, A_3$ then $\vect{a}_{A_1} = \mat{R}_1 \vect{a}_{A_2}$ and $\vect{a}_{A_3} = \mat{R}_3 \vect{a}_{A_2}$ due to functional dependencies. Furthermore, if we concatenate particular matrices $\mat{R}= \begin{bmatrix}
\mat{R}_1 \\ \mat{R}_3
\end{bmatrix}
$  and 
$
\vect{a}_{A_1A_3}= \begin{bmatrix}
\vect{a}_{A_1} \\
\vect{a}_{A_3}
\end{bmatrix}
$, 
we get 
\begin{equation}
\vect{a}_{A_1A_3}=\mat{R}\vect{a}_{A_2}
\label{eq:RCHange}
\end{equation}
 where $\vect{a}_{A_2}, \vect{a}_{A_1}, \vect{a}_{A_3}$ are column vectors corresponding to tuples. Further, we will use notation $\vect{a}_i = \vect{a}_{A_i}$. Approximation of output based on $\pmb{\theta} s$ is:
\begin{eqnarray}
\hat{y} &=& 
\sum_{ i\in [n-1] \setminus \{ 1, 2,3 \} \cup \{0\}} \langle \pmb{\theta}_i, \vect{a}_i \rangle + \langle \pmb{\theta}_1 , \vect{a}_1 \rangle  + \langle \pmb{\theta}_2 , \vect{a}_2 \rangle + \langle \pmb{\theta}_3, \vect{a}_3 \rangle \label{eq:initial}\\
&=&
\sum_{ i\in [n-1] \setminus \{ 1, 2,3 \} \cup \{0\}} \langle \pmb{\theta}_i, \vect{a}_i \rangle + \langle \pmb{\theta}_{13},\vect{a}_{13} \rangle  + \langle \pmb{\theta}_2, \vect{a}_2 \rangle \label{eq:concat} \\
&=&
\sum_{ i\in [n-1] \setminus \{ 1, 2,3 \}\cup \{0\}} \langle \pmb{\theta}_i, \vect{a}_i \rangle + \langle \pmb{\theta}_{13},\mat{R} \vect{a}_2 \rangle  + \langle \pmb{\theta}_2 ,\vect{a}_2 \rangle \label{eq:RNew} \\
&=&
\sum_{ i\in [n-1] \setminus \{ 1, 2,3 \}\cup \{0\}} \langle \pmb{\theta}_i, \vect{a}_i \rangle +  \transpose{\pmb{\theta}_{13}} \cdot (\mat{R} \vect{a}_2)  + \transpose{\pmb{\theta}_2 }\cdot \vect{a}_2 \label{eq:DotDef} \\
&=&
\sum_{ i\in [n-1] \setminus \{ 1, 2,3 \}\cup \{0\}} \langle \pmb{\theta}_i, \vect{a}_i \rangle + (\transpose{\pmb{\theta}_{13}} \mat{R})\cdot \vect{a}_2 + \transpose{\pmb{\theta}_2 }\cdot \vect{a}_2  \label{eq:associativity} \\
&=&
\sum_{ i\in [n-1] \setminus \{ 1, 2,3 \}\cup \{0\}} \langle \pmb{\theta}_i, \vect{a}_i \rangle + (\transpose{\pmb{\theta}_{13}} \mat{R}+ \transpose{\pmb{\theta}_2 })\cdot \vect{a}_2  \label{eq:distributivity} \\
&=&
\sum_{ i\in [n-1]\setminus \{ 1, 2,3 \} \cup \{0\}} \langle \pmb{\theta}_i, \vect{a}_i \rangle +   \langle \transpose{\mat{R}} \pmb{\theta}_{13} +\pmb{\theta}_2 ,\vect{a}_2 \rangle  \label{eq:approximation}
\end{eqnarray}
In the equation \ref{eq:initial} we used formula for prediction of output based on input using learned parameters. In the equation \ref{eq:concat} we concatenated parameters and input tuples into column vectors. In the equation \ref{eq:RNew} we changed variable $\vect{a}_{13}$ using equation \ref{eq:RCHange}. 
In the equation \ref{eq:DotDef} we used the fact that $\langle \vect{a}, \vect{b} \rangle = \transpose{\vect{a}} \vect{b}$. In the equation \ref{eq:associativity} we used an associativity of matrix multiplication. In the equation \ref{eq:distributivity} we used an distributivity over matrix addition. In the \ref{eq:approximation} we used the fact  $\langle \vect{a}, \vect{b} \rangle = \transpose{\vect{a}} \vect{b}$ and the fact  $\transpose{(\vect{a} \cdot \vect{b})} = \transpose{\vect{b}} \cdot \transpose{\vect{a}}$
\\
Let $\pmb{\gamma}_2 = \transpose{\mat{R}}\pmb{\theta}_{13} + \pmb{\theta}_2$ and let $\pmb{\gamma}_j = \pmb{\theta}_j, j \in \{0, 4, ..., n-1\}$. $\pmb{\theta}_1, \pmb{\theta}_3$ won't appear in the square loss function because they don't appear in $\hat{y}$ as we see in the equation \ref{eq:approximation}. They still appear in $\ell_2$ regularizer. The square loss for each input (tuple in join) is:
$$
\left(\sum_{ i\in [n-1]\setminus \{  1,3 \} \cup \{0\}} \langle \pmb{\gamma}_i, \vect{a}_i \rangle - \vect{a}_n\right)^2 \label{eq:approximation}
$$
In the $\ell_2$ regularizer we have:
$$\ell_2 = \frac{\lambda}{2} \sum_{j \in \{0, 4, 5, ..., n \} } ||\pmb{\gamma}_j||_2^2 + ||\pmb{\theta}_{13} ||^2_2 +  ||\pmb{\gamma}_2 - \transpose{\mat{R}}\pmb{\theta}_{13}  ||_2^2$$
By optimizing out (we reach optimal value) $\pmb{\theta}_{13}$ from regularizer we get 
\begin{align}
\frac{1}{\lambda} \frac{\partial \ell_2}{\partial \pmb{\theta}_{13}} &=&
0 + 
\frac{\partial \transpose{\pmb{\theta}_{13}}\pmb{\theta}_{13}}{\partial \pmb{\theta}_{13}} + 
\frac{\partial \transpose{(\pmb{\gamma}_2 - 
\transpose{\mat{R}}\pmb{\theta}_{13})}(\pmb{\gamma}_2 - \transpose{\mat{R}}\pmb{\theta}_{13})}{2\partial \pmb{\theta}_{13}} \label{eq:simpleExp}\\
&=& \frac{\partial \transpose{\pmb{\theta}_{13}}\pmb{\theta}_{13}}{2\partial \pmb{\theta}_{13}} + \frac{\partial \transpose{\pmb{\gamma}_2}\pmb{\gamma}_2}{2\partial \pmb{\theta}_{13}}
-2 \frac{\partial (\transpose{\pmb{\theta}_{13}}\mat{R}\pmb{\gamma}_2)}{2\partial \pmb{\theta}_{13}}
+ \frac{\partial (\transpose{\pmb{\theta}_{13}}\mat{R}\transpose{\mat{R}}\pmb{\theta}_{13})}{2\partial \pmb{\theta}_{13}} \label{eq:simpleCalc}
\\
&=& \pmb{\theta}_{13} + 0 - \mat{R}\pmb{\gamma}_2 +  \mat{R}\transpose{\mat{R}}\pmb{\theta}_{13} \label{eq:derRules}\\
&=&(\mat{I}_{13} + \mat{R}\transpose{\mat{R}})\pmb{\theta}_{13} + \mat{R}\pmb{\gamma}_2 =0
\label{eq:optimized}
\end{align}
In the equation \ref{eq:simpleExp} we rewrote expressions for norms using rule $|\vect{v}|_2^2 = \transpose{\vect{v}}{\vect{v}}$. In the equation \ref{eq:simpleCalc} we used simple matrix operations (multiply and add). In the equation \ref{eq:derRules} we used properties $\gradient{\vect{x}}(\transpose{\vect{x}}\mat{A}\vect{x})=\mat{A}\vect{x} + \transpose{\mat{A}}\vect{x}$, $\gradient{\vect{x}}(\transpose{\vect{x}}\vect{x})=2\vect{x}$, $\gradient{\vect{x}}(\transpose{\vect{c}} \vect{x}) = \vect{c}$. 
By further solving the equation \ref{eq:optimized} we get:
$$\pmb{\theta}_{13} = (\mat{I}_{13} + \mat{R}\transpose{\mat{R}})^{-1}\mat{R}\pmb{\gamma}_2$$
Finally $\ell_2$ regularizer is:
$$\frac{\lambda}{2} \left(\sum_{j \in \{0, 4, 5, ..., n-1 \} } ||\pmb{\gamma}_j||_2^2 + \langle (\mat{I}_2 + \transpose{\mat{R}}\mat{R})^{-1} \pmb{\gamma}_2 , \pmb{\gamma}_2 \rangle \right)$$ 
The final derivation is the same as in \cite[page 21 C.1 appendix]{abo2018database} just in indices of parameters instead of city is $2$ and instead of country is $13$.
\\
The reparametrized loss function is:

$$
\frac{1}{2|D|}\sum_{\vect{a}\in D}\left(\sum_{ i\in [n-1]\setminus \{  1,3 \} \cup \{0\}} \langle \pmb{\gamma}_i, \vect{a}_i \rangle - \vect{a}_n\right)^2 + \frac{\lambda}{2} \left(\sum_{j \in \{0, 4, 5, ..., n-1 \} } ||\pmb{\gamma}_j||_2^2 + \langle (\mat{I}_2 + \transpose{\mat{R}}\mat{R})^{-1} \pmb{\gamma}_2 , \pmb{\gamma}_2 \rangle \right) 
$$
We have reduced of  categorical variables from three to one, which reduces $n_d$ to $n_w = n - 3 + |Dom_{A_2}|$ in the case of listing representation
$$\mathcal{O}(N^\frac{n}{2} + (n_w)^2 \cdot N^\frac{n}{2} + n_w \cdot  N^\frac{n}{2} + n_w^2 \cdot t) $$
In the case of FAQs we have removed the most time consuming FAQs, more precisely in the case of 2 categorical variables, we just need to evaluate FAQ for one variable. Thus, if sizes of domains are big in comparison to database size, constant in complexity is reduced drastically.
\item[2.]
\item[2.1]

The OuMv problem is:
\begin{itemize}
\item 
Input an $n \times n$ Boolean matrix \mat{M} and $n$ pairs $(\vect{u}_1, \vect{v}_1), ..., (\vect{u}_n, \vect{v}_n)$ of Boolean column-vectors of size $n$ arriving one after the other. 
\item 
	Goal is after seeing each pair $(\vect{u}_r, \vect{v}_r)$ to output $\transpose{\vect{u}_r}\mat{M}\vect{v}_r$.
\end{itemize}
\begin{conjecture}\label{conj:OUVM}
For any $\gamma > 0$, there is no algorithm that solves the OuMv problem in time $\mathcal{O}(n^{3-\gamma})$.
\end{conjecture}
Let us assume there is an algorithm $\mathcal{A}$ which maintains 4-cycle detection with update time $\mathcal{O}(N^{\frac{1}{2}-\gamma})$, pre-processing time $\mathcal{O}(N^2)$, and answer time $\mathcal{O}(1)$ over a database of size $N$, unless the OuMv conjecture fails. 
The algorithm  $\mathcal{B}$ using algorithm $\mathcal{A}$ which solves OuMv in subcubic time is:
\begin{itemize}
\item[(1)] 
Insert at most $n^2$ tuples in $R$ such that $R(A, B) = \{(i, j) | \mat{M}(i, j) = 1\}$ and in $T$ such that $T(C, D) = \{(j, i) | \mat{M}(i, j) = 1\}$ ;
\item[(2)] In each round $r \in [n]$:
\begin{itemize}
\item[(2.1)] Delete all tuples in in $S, W$;
\item[(2.2)]  Insert at most $2n$ tuples into $S,  W$ 
$S(B, C) = \{(j, j) | \vect{v}_r(j) = 1\}$
$W(D, A) = \{(i, i) | \vect{u}_r(i) = 1\}$;
\item[(2.3)] Check $R \bowtie S \bowtie T \bowtie W \neq \emptyset$. This holds only iff $\transpose{\vect{u}}_r\mat{M}\vect{v}_r=1$
$
\exists i, j \in[n] \text { with } \vect{u}_{r}(i)=1, \mat{M}(i, j)=1, \text { and } \vect{v}_{r}(j)=1
$.
Time analysis:
\end{itemize}
\end{itemize}
There are at most $n^2$ tuples inserted at the step (1) and $2n$ at the step 2, thus the size of database is at most $O(n^2)$, because tuples from the step $2$ are deleted in each round. 
A complexity of the step (1) of algorithm $\mathcal{B}$ is:
$$\mathcal{O}(n^2 \cdot (n^2)^{\frac{1}{2} -\gamma}) =
\mathcal{O}(n^2 \cdot n^{1- 2 \gamma }) = \mathcal{O}(n^{3-2\gamma})$$
Here we calculated complexity of inserting $n^2$  tuples to the database.  
A complexity of the step (2) of algorithm $\mathcal{B}$ is:
$$\mathcal{O}(4n \cdot (n^2)^{\frac{1}{2} -\gamma} + 1) = \mathcal{O}(4 n^{2-2\gamma} + 1 ) = \mathcal{O}(n^{2-2\gamma})$$
The analysis of the step (2) takes into consideration that there are at most $2n$  tuples to be deleted and inserted from $S, W$ ($\vect{u}_r$ and $\vect{v}_r$ are filled with ones).
Time for $n$ rounds in the step (2) is:
$$\mathcal{O}(n \cdot n^{2-2\gamma})=\mathcal{O}(n^{3-2\gamma})$$ 
Summing complexities of steps (1) and 2) we get that an overall time complexity is $\mathcal{O}(n^{3-2\gamma})$, which is in contradiction to the claim from the conjecture \ref{conj:OUVM}. 

\item[2.2]
Let us call the first variables in relations $R, S, T, W$ variables $A, B, C, D$, respectively . Similarly, let us call the second variables in relations $R, S, T, W$: $B, C, D, A$, respectively. 
Let us partition each relation in 4 partitions based on whether the variable is light or heavy in that relation. In case of a relation, we have 4 possible partitioning. If type of the second variable is not stated, it will include both heavy and light values. 
We will use the same strategy used for triangle query with auxiliary views and updates of a count by iterating different type of partitions. 
We will only give an update strategy for cycle count when relation $R(A, B)$ is updated, other relations are updated analogously. 
\\
In the table \ref{tab:Comp} we have complexities for different types of partitions of variables, along with update strategies for the cycle count. The column evaluation strategy will be explained on an example of the first row:
Iterate through $c$ values in relation $S_L$ corresponding to updated $b'$, for each of those $c$ values iterate through corresponding $d$ in $T_L$, for each of those $d$ values iterate though a' (check if exists a tuple $(d, a')$ ) in $W_L$, for each of the iterated values add product of tuples from corresponding relations to the cycle count. In this case we would add for each iterated values a product: $\delta R_*(a', b') \cdot S_L(b', c) T_L(c, d) W_L(d, a')$. Since we have at most $|D|^\varepsilon$ $c$ values corresponding to $b'$ in $S_L$, and $|D|^\varepsilon$ $d$ values for each of $c$ values in $T_L$ we have at most $O|D|^{2\varepsilon}$ possible iterations. After we calculated sum of all products, we add this to the cycle count number. Also, we have to update relation $R$. 
\\
In some cases, where previous logic does not work because a number of iterations is at least linear, we have to keep an auxiliary view of cycle counts to which tuple $(a, b)$ belongs (excluding multiple of numbers of $(a, b)$). Updating auxiliary view in this case when $(a, b)$ is not needed. Although, we need to update cycle count which is updated by adding $\delta R_*(a', b') V(...)$. In addition, we might need to update other views, using the same logic as in the update to non-views. Calculation of initial views/cycle count is $\mathcal{O}(N^2)$ because we need to do a join over a database, which is at most $\mathcal{O}(N^2)$, and list  all possible combinations of $(a,b)$ which is at most $\mathcal{O}(N^2)$. 
\renewcommand{\figurename}{Table}
\begin{figure}	
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
Type of S & Type of T & Type of W & Time & Evaluation from left to right \\ 
 \hline 
%1
L & L & L & $\mathcal{O}(|D|^{2\varepsilon})$ &
$\delta R_*(a', b') \cdot \sum_c S_L(b', c) \sum_d T_L(c, d) W_L(d, a')$ \\ 
\hline 
%2
L & L & H & $\mathcal{O}(|D|^{2\varepsilon})$ & $\delta R_*(a', b') \cdot \sum_c S_L(b', c) \sum_d T_L(c, d) W_H(d, a')$ \\ 
\hline 
%3
L & H  & LL & $\mathcal{O}(|D|^{2\varepsilon})$ & $\delta R_*(a', b') \cdot \sum_c S_L(b', c) \sum_d W_{LL}(d, a') T_{H}(c, d) $ \\ 
\hline 
L & H & LH & $\mathcal{O}(1)$ & $\delta R_*(a', b') \cdot V_{S_LT_HW_{LH}}(b', a') $ \\ 
\hline 
%4
L & H & HL & $\mathcal{O}(|D|^{2\varepsilon})$ &  $\delta R_*(a', b') \cdot \sum_c S_L(b', c) \sum_d W_{HL}(d, a') T_{H}(c, d) $ \\ 
\hline 
L & H & HH & $\mathcal{O}(1)$ & $\view{L}{H}{HH}$ \\ 
\hline 
%5
H & LL & LL &  $\mathcal{O}(|D|^{2\varepsilon})$ &
 $\delta R_*(a', b') \cdot \sum_d W_{LL}(d, a') \sum_c T_{LL}(c, d) S_H(b', c)   $\\ 
\hline 
H & LH & LL & $\mathcal{O}(1)$ & $\view{H}{LH}{LL}$  \\ 
\hline 
H & L & LH & $\mathcal{O}(1)$  & $\view{H}{L}{LH}$ \\ 
\hline 
%6
H & LL & HL & $\mathcal{O}(|D|^{2\varepsilon})$ &  $\delta R_*(a', b') \cdot \sum_d W_{HL}(d, a') \sum_c T_{LL}(c, d) S_H(b', c)$
\\ 
\hline 
H & LH & HL & $\mathcal{O}(1)$ & $\view{H}{LH}{HL}$ \\ 
\hline 
H & L & HH & $\mathcal{O}(1)$ & $\view{H}{L}{HH}$ \\ 
%7
\hline 
H & HL & LL & $\mathcal{O}(|D|^{2\varepsilon})$ & $\delta R_*(a', b') \cdot \sum_d W_{LL}(d, a') \sum_c T_{HL}(c, d) S_H(b', c)$\\ 
\hline 
H & HH & LL & $\mathcal{O}(1)$ & $\view{H}{HH}{LL}$ \\ 
\hline 
H & H & LH, HH & $\mathcal{O}(1)$ & $\view{H}{H}{LL, HH}$ \\ 
\hline 
%8
H & HL & HL & $\mathcal{O}(|D|^{2\varepsilon})$ & $\delta R_*(a', b') \cdot \sum_d W_{HL}(d, a') \sum_c T_{HL}(c, d) S_H(b', c)$\\ 
\hline 
H & HH & HL & $\mathcal{O}(1)$ & $\view{H}{HH}{HL}$ \\ 
\hline 
H & H & HH & $\mathcal{O}(1)$ & $\view{H}{H}{HH}$ \\ 
\hline 
\end{tabular} 
\caption{Table of evaluation strategies for update of 4-cycle count}
\end{center}
\label{tab:Comp}
\end{figure}

\end{enumerate}


\bibliographystyle{IEEEtranN}
\bibliography{research}

\end{document}